<!DOCTYPE html>

<html lang="en">

<head>
  <meta content="HSMR" name="title" />
  <meta content="This paper recoveries biomechanically accurate human pose." name="description" />
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <meta content="English" name="language" />
  <meta content="Yan Xia" name="author" />
  <title>VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</title>
  <!-- Bootstrap -->
  <script src="js/jquery-3.4.1.min.js"></script>
  <script src="js/bootstrap-4.4.1.js"></script>
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet" />

  <!-- Add-ones -->
  <script src="js/video-carousel.js"></script>
  <script src="js/highlight-kw.js"></script>
  <link href="css/highlight-kw.css" rel="stylesheet" />
  <link href="css/theme.css" rel="stylesheet"/>

  <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet" />
  <!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Philosopher:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css" rel="stylesheet" />
  
  <!-- Favicon -->
  <link rel="icon" type="image/png" href="assets/vlar1_logo.png">

  <script src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js"></script>

  </link>
</head>

<body>
  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h1 style="font-weight: bold">
              <b>
                <span style="color: #98BACB">VLA-</span><span style="color: #CAB6A5">R1</span><br>
              </b>
            </h1>
            <h2>Enhancing <span style="color: #CAB6A5">Reasoning</span> in <span style="color: #98BACB">Vision-Language-Action</span> Models</h2>
            <!-- <h4 style="color:#5a6268;">Conference 2026</h4> -->
            <hr>
            <a href="#">Angen Ye</a><sup>1*</sup>&nbsp;
            <a href="https://steve-zeyu-zhang.github.io/">Zeyu Zhang</a><sup>1*</sup>&nbsp;
            <a href="#">Boyuan Wang</a><sup>1*</sup>&nbsp;
            <a href="#">Xiaofeng Wang</a><sup>1</sup>&nbsp;
            <a href="#">Dapeng Zhang</a><sup>2</sup>&nbsp;
            <a href="http://www.zhengzhu.net/">Zheng Zhu</a><sup>1†</sup>
            <p>
              <sup>1</sup>GigaAI &nbsp;&nbsp;&nbsp;
              <sup>2</sup>CUHK-Shenzhen
               <br>
               <sup>*</sup>Equal contribution. &nbsp;&nbsp;&nbsp;
               <sup>†</sup>Corresponding author.
            </p>
            <div class="row justify-content-center">

              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="#" role="button">
                    <i class="fas fa-file-pdf"></i>
                    <b>Paper</b>
                  </a>
                </p>
              </div>

              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="https://github.com/GigaAI-research/VLA-R1" role="button">
                    <i class="fas fa-code"></i>
                    <b>Code</b>
                  </a>
                </p>
              </div>

              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="#" role="button">
                    <i class="fa fa-gamepad"></i>
                    <b>Model</b>
                  </a>
                </p>
              </div>

              <div class="column">
                <p class="mb-5">
                  <a class="btn btn-large btn-light" href="#" role="button">
                    <i class="fa fa-play"></i>
                    <b>Dataset</b>
                  </a>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h3>TL;DR</h3>
          <hr style="margin-top: 0px" />

          <p class="text-left">
            <strong>VLA-R1</strong> is a reasoning-enhanced vision–language–action model that enables step-by-step reasoning and robust action execution across diverse tasks and domains.<br>
            (1) A RL-based reasoning optimization scheme that strengthens step-by-step reasoning and execution;<br>
            (2) VLA-CoT-13K, a high-quality dataset with explicit chain-of-thought supervision; <br>
            (3) Comprehensive benchmarks, simulation, and real-world evaluation demonstrating the superiority of VLA-R1. <br>
          </p>

          <div id="TeaserVideos" style="text-align: center;">
            <video autoplay muted loop playsinline controls style="max-width:80%; border-radius:8px;">
              <source src="videos/teaser.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <br>          
          

        </div>
      </div>
    </div>
  </section>


  <!-- Methods -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">

          <h3>Method</h3>
          <hr style="margin-top: 0px" />

          <div style="display: flex; justify-content: center;">
            <img src="assets/arch2_page-0001.jpg" style="max-width:80%; border-radius:8px;" />
          </div>

          <p style="text-align: center; max-width:80%; margin: 0 auto;">
            <strong>Overall architecture of VLA-R1.</strong>
            The system operates in two stages. Stage I applies supervised fine-tuning (SFT) augmented with Chain-of-Thought (CoT) supervision to endow the multimodal model with foundational reasoning, enabling it to generate trajectories or affordance regions conditioned on images and instructions; a downstream control stack then converts these outputs into joint-level robot commands. Stage II introduces reinforcement learning with verifiable rewards (GRPO) to further refine both reasoning quality and action execution, yielding more robust cross-task generalization and resilience in complex scenarios.
          </p>


        </div>
      </div>
    </div>
  </section>



  <!-- Visualization Results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">

          <h3>Visualization</h3>
          <hr style="margin-top: 0px" />

          <center><h4>Real-World Results</h4></center>

        
            <!-- Video grid -->
            <div class="video-grid">
            <!-- 2 video blocks (2 × 1) -->

                <div class="video-block">
                    <video controls autoplay muted loop>
                    <source src="videos/1.mp4" type="video/mp4">
                    </video>
                    <div class="figure-caption">Pick up the yellow bowl. Move the yellow bowl to the white basket.</div>
                </div>

                <div class="video-block">
                    <video controls autoplay muted loop>
                    <source src="videos/2.mp4" type="video/mp4">
                    </video>
                    <div class="figure-caption">Grasp the plush toy on the table.</div>
                </div>

            </div>

            <!-- Video grid -->
            <div class="video-grid">
              <!-- 2 video blocks (2 × 1) -->
  
                  <div class="video-block">
                      <video controls autoplay muted loop>
                      <source src="videos/3.mp4" type="video/mp4">
                      </video>
                      <div class="figure-caption">Move the bread into the microwave.</div>
                  </div>
  
                  <div class="video-block">
                      <video controls autoplay muted loop>
                      <source src="videos/4.mp4" type="video/mp4">
                      </video>
                      <div class="figure-caption">Put the bread into the bread box.</div>
                  </div>
  
              </div>

          
            <!-- Video grid -->
            <div class="video-grid">
              <!-- 2 video blocks (2 × 1) -->

                  <div class="video-block">
                      <video controls autoplay muted loop>
                      <source src="videos/5.mp4" type="video/mp4">
                      </video>
                      <div class="figure-caption">Pick the yellow bowl.</div>
                  </div>

                  <div class="video-block">
                      <video controls autoplay muted loop>
                      <source src="videos/6.mp4" type="video/mp4">
                      </video>
                      <div class="figure-caption">Grip the yellow bowl. Grip the red bowl. Grip the blue bowl. Grip the white bowl. Grip the green bowl.</div>
                  </div>

              </div>


              <!-- Video grid -->
            <div class="video-grid">
              <!-- 2 video blocks (2 × 1) -->

                  <div class="video-block">
                      <video controls autoplay muted loop>
                      <source src="videos/7.mp4" type="video/mp4">
                      </video>
                      <div class="figure-caption">Grasp the green bowl.</div>
                  </div>

                  <div class="video-block">
                      <video controls autoplay muted loop>
                      <source src="videos/8.mp4" type="video/mp4">
                      </video>
                      <div class="figure-caption">Transfer the bananas into the brown basket.</div>
                  </div>

              </div>
  
          

          <br>
          <hr />

          <center><h4>Simulated Results</h4></center>

          <!-- Video grid -->
          <div class="video-grid">
            <!-- 2 video blocks (2 × 1) -->

                <div class="video-block">
                    <video controls autoplay muted loop>
                    <source src="sim/1.mp4" type="video/mp4">
                    </video>
                    <div class="figure-caption">Pick up the green bowl.</div>
                </div>

                <div class="video-block">
                    <video controls autoplay muted loop>
                    <source src="sim/2.mp4" type="video/mp4">
                    </video>
                    <div class="figure-caption">Pick the black box.</div>
                </div>

            </div>


            <!-- Video grid -->
          <div class="video-grid">
            <!-- 2 video blocks (2 × 1) -->

                <div class="video-block">
                    <video controls autoplay muted loop>
                    <source src="sim/3.mp4" type="video/mp4">
                    </video>
                    <div class="figure-caption">Grab the orange box.</div>
                </div>

                <div class="video-block">
                    <video controls autoplay muted loop>
                    <source src="sim/4.mp4" type="video/mp4">
                    </video>
                    <div class="figure-caption">Move the orange box to the empty spot on the table-top.</div>
                </div>

            </div>
          

        </div>
      </div>
    </div>
  </section>


  <!-- Citing -->
  <div class="container">
    <div class="row">
      <div class="col-md-12">

        <h3>Citation</h3>
        <hr style="margin-top: 0px" />
        <pre class="selectable" style="background-color: #e9eeef; padding: 1.5em 1.5em; border-radius: 15px"><code>@article{song2025maniplvm,
  title={Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large vision-language models},
  author={Song, Zirui and Ouyang, Guangxian and Li, Mingzhe and Ji, Yuheng and Wang, Chenxi and Xu, Zixiang and Zhang, Zeyu and Zhang, Xiaoqing and Jiang, Qian and Chen, Zhenhao and others},
  journal={arXiv preprint arXiv:2505.16517},
  year={2025}
}</code></pre>
        <hr />
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom: 10px">
    VLA-R1: Enhancing Reasoning in Vision-Language-Action Models
  </footer>

</body>

</html>
